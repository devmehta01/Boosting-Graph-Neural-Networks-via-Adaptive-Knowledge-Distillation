# Boosting-Graph-Neural-Networks-via-Adaptive-Knowledge-Distillation
Enhancing Graph Neural Network Performance through Adaptive Knowledge Distillation and Weight Augmentation to optimize their classification and link prediction capabilities.

## Usage:
The models are already trained and stored for node and link prediction. The student.sh file needs to be run to train the student. The contents of the student.sh file can be modified to change the task (node or link prediction), to change the model used for the student and teacher, or to adjust other parameters.
If you want to retrain the teacher model, the teacher.sh file needs to be run.
